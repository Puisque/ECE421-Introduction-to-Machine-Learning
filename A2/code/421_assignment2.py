# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15DEI9RMj2gqoF4UEsG5sQ64JhL3OCH_v
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import time
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Load the data
def loadData():
    with np.load("notMNIST.npz") as data:
        Data, Target = data["images"], data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Implementation of a neural network using only Numpy - trained using gradient descent with momentum
def convertOneHot(trainTarget, validTarget, testTarget):
    newtrain = np.zeros((trainTarget.shape[0], 10))
    newvalid = np.zeros((validTarget.shape[0], 10))
    newtest = np.zeros((testTarget.shape[0], 10))

    for item in range(0, trainTarget.shape[0]):
        newtrain[item][trainTarget[item]] = 1
    for item in range(0, validTarget.shape[0]):
        newvalid[item][validTarget[item]] = 1
    for item in range(0, testTarget.shape[0]):
        newtest[item][testTarget[item]] = 1
    return newtrain, newvalid, newtest

trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()
print(trainData.shape)
print(trainTarget)

train_target, valid_target, test_target = convertOneHot(trainTarget, validTarget, testTarget)
trainData = trainData.reshape(trainData.shape[0], -1)
validData = validData.reshape(validData.shape[0], -1)
testData = testData.reshape(testData.shape[0], -1)

def shuffle(trainData, trainTarget):
    np.random.seed(421)
    randIndx = np.arange(len(trainData))
    target = trainTarget
    np.random.shuffle(randIndx)
    data, target = trainData[randIndx], target[randIndx]
    return data, target

"""#1    Neural Networks using Numpy

## 1.1    Helper Functions
"""

def relu(x):
    relu_x = np.maximum(x, 0)
    return relu_x

def softmax(x):
    x = x - np.max(x, axis=1, keepdims=True)
    sm_x = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)
    return sm_x

def computeLayer(X, W, b):
    return np.dot(X, W) + b

def CE(target, prediction):
    ce = (-1) * np.mean(np.multiply(target, np.log(prediction)))
    return ce

def gradCE(target, prediction):
    return (prediction - target)

"""## 1.2    Backpropagation Derivation"""

def dL_dW2(hidden_out, target, prediction):
    return np.dot(hidden_out.T, gradCE(target, prediction))

def dL_db2(target, prediction):
    return np.dot(np.ones((1, target.shape[0])), gradCE(target, prediction))

def dL_dW1(before_relu, input_data, target, prediction, W_2):
    before_relu[before_relu >= 0] = 1
    before_relu[before_relu < 0] = 0
    gradient_W1 = np.dot(input_data.T, (before_relu * np.dot(gradCE(target, prediction), W_2.T)))
    return gradient_W1

def dL_db1(before_relu, target, prediction, W_2):
    before_relu[before_relu >= 0] = 1
    before_relu[before_relu < 0] = 0
    gradient_bias1 = np.dot(np.ones((1, before_relu.shape[0])), (before_relu * np.dot(gradCE(target, prediction), W_2.T)))
    return gradient_bias1

"""## 1.3    Learning"""

def testAccuracy(W1, W2, b1, b2, test_data, test_target):
  corr = 0
  test_accuracy = np.zeros(test_data.shape[0])

  for idx in range(test_data.shape[0]):
    test_hidden_in = computeLayer(test_data[idx], W1, b1)
    test_hidden_out = relu(test_hidden_in)

    test_output_in = computeLayer(test_hidden_out, W2, b2)
    test_prediction = softmax(test_output_in)

    test_prediction_label = np.argmax(test_prediction, axis=1)
    test_true_label = np.argmax(test_target[idx], axis=0)
    corr += sum(test_true_label == test_prediction_label)
    test_accuracy[idx] = corr / (idx + 1)
    
  return test_accuracy

def train(train_data, train_target, valid_data, valid_target, test_data, test_target, num_hidden_units, epochs, gamma, alpha):

    W_1 = np.random.normal(0, np.sqrt(2/(28*28+num_hidden_units)), (28*28, num_hidden_units))
    W_2 = np.random.normal(0, np.sqrt(2/(num_hidden_units+10)), (num_hidden_units, 10))

    bias1 = np.zeros((1, num_hidden_units))
    bias2 = np.zeros((1, 10))

    train_loss = np.zeros(epochs)
    train_accuracy = np.zeros(epochs)
    valid_loss = np.zeros(epochs)
    valid_accuracy = np.zeros(epochs)
    test_loss = np.zeros(epochs)
    test_accuracy = np.zeros(epochs)
   
    new_v_W_1 = np.zeros((28*28, num_hidden_units))
    new_v_W_2 = np.zeros((num_hidden_units, 10))

    new_v_b_1 = np.zeros((1, num_hidden_units))
    new_v_b_2 = np.zeros((1, 10))

    for i in range(epochs):
      # train
      train_hidden_in = computeLayer(train_data, W_1, bias1)
      train_hidden_out = relu(train_hidden_in)

      train_output_in = computeLayer(train_hidden_out, W_2, bias2)
      train_prediction = softmax(train_output_in)
      train_loss[i] = CE(train_target, train_prediction)
      train_prediction_label = np.argmax(train_prediction, axis=1)
      train_true_label = np.argmax(train_target, axis=1)
      train_accuracy[i] = (np.sum(train_true_label == train_prediction_label) / train_data.shape[0])
      

      # valid
      valid_hidden_in = computeLayer(valid_data, W_1, bias1)
      valid_hidden_out = relu(valid_hidden_in)

      valid_output_in = computeLayer(valid_hidden_out, W_2, bias2)
      valid_prediction = softmax(valid_output_in)
      valid_loss[i] = CE(valid_target, valid_prediction)
      valid_prediction_label = np.argmax(valid_prediction, axis=1)
      valid_true_label = np.argmax(valid_target, axis=1)
      valid_accuracy[i] = (np.sum(valid_true_label == valid_prediction_label) / valid_data.shape[0])

      # test
      test_hidden_in = computeLayer(test_data, W_1, bias1)
      test_hidden_out = relu(test_hidden_in)

      test_output_in = computeLayer(test_hidden_out, W_2, bias2)
      test_prediction = softmax(test_output_in)
      test_loss[i] = CE(test_target, test_prediction)
      test_prediction_label = np.argmax(test_prediction, axis=1)
      test_true_label = np.argmax(test_target, axis=1)
      test_accuracy[i] = (np.sum(test_true_label == test_prediction_label) / test_data.shape[0])
      
      # update
      new_v_W_1 = gamma*new_v_W_1 + alpha*dL_dW1(train_hidden_in, train_data, train_target, train_prediction, W_2)
      W_1 = W_1 - new_v_W_1

      new_v_b_1 = gamma*new_v_b_1 + alpha*dL_db1(train_hidden_in, train_target, train_prediction, W_2)
      bias1 = bias1 - new_v_b_1

      new_v_W_2 = gamma*new_v_W_2 + alpha*dL_dW2(train_hidden_out, train_target, train_prediction)
      W_2 = W_2 - new_v_W_2

      new_v_b_2 = gamma*new_v_b_2 + alpha*dL_db2(train_target, train_prediction)
      bias2 = bias2 - new_v_b_2

      print("Epoch =", i+1, "train accuracy =", train_accuracy[i], "validation accuracy =", valid_accuracy[i])

    train_label = "Train"
    valid_label = "Validation"
    test_label = "Test"
    plt.title("Train vs Validation vs Test Loss")
    n = len(train_loss) # number of epochs
    plt.plot(range(1,n+1), train_loss, label=train_label)
    plt.plot(range(1,n+1), valid_loss, label=valid_label)
    plt.plot(range(1,n+1), test_loss, label=test_label)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.grid()
    plt.show()

    train_label = "Train"
    valid_label = "Validation"
    test_label = "Test"
    plt.title("Train vs Validation vs Test Accuracy")
    n = len(train_accuracy) # number of epochs
    plt.plot(range(1,n+1), train_accuracy, label=train_label)
    plt.plot(range(1,n+1), valid_accuracy, label=valid_label)
    plt.plot(range(1,n+1), test_accuracy, label=test_label)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.grid()
    plt.show()

    #test_accuracy = testAccuracy(W_1, W_2, bias1, bias2, test_data, test_target)
    #plt.title("Test Accuracy")
    #n = len(test_accuracy)
    #plt.plot(range(1,n+1), test_accuracy)
    #plt.xlabel("Number of examples")
    #plt.ylabel("Accuracy")
    #plt.grid()
    #plt.show()
    print(f"Final test accuracy is: {test_accuracy[-1]}")

# H = 1000 units
start_time1000 = time.time()

train(trainData, train_target, validData, valid_target, testData, test_target, 1000, 200, 0.9, 1e-5)

end_time1000 = time.time()
diff1000 = end_time1000 - start_time1000

print(f"Time elapsed: {diff1000}")

"""## 1.4    Hyperparameter Investigation"""

# H = 100 units
start_time100 = time.time()

train(trainData, train_target, validData, valid_target, testData, test_target, 100, 200, 0.9, 1e-5)

end_time100 = time.time()
diff100 = end_time100 - start_time100

print(f"Time elapsed: {diff100}")

# H = 500 units
start_time500 = time.time()

train(trainData, train_target, validData, valid_target, testData, test_target, 500, 200, 0.9, 1e-5)

end_time500 = time.time()
diff500 = end_time500 - start_time500

print(f"Time elapsed: {diff500}")

# H = 2000 units
start_time2000 = time.time()

train(trainData, train_target, validData, valid_target, testData, test_target, 2000, 200, 0.9, 1e-5)

end_time2000 = time.time()
diff2000 = end_time2000 - start_time2000

print(f"Time elapsed: {diff2000}")

"""# 2    Neural Networks in Tensorflow

## 2.1    Model implementation
"""

def CNN(x, y, learning_rate, beta1, beta2, epsilon):
    tf.set_random_seed(421)
    x = tf.placeholder("float", [None, 28,28,1])
    y = tf.placeholder("float", [None, 10])
    weights = {
        'w1': tf.get_variable('w1', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), 
        'w2': tf.get_variable('w2', shape=(14*14*32,784), initializer=tf.contrib.layers.xavier_initializer()),  
        'w3': tf.get_variable('w3', shape=(784,10), initializer=tf.contrib.layers.xavier_initializer()), 
    }
    biases = {
        'b1': tf.get_variable('b1', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),
        'b2': tf.get_variable('b2', shape=(784), initializer=tf.contrib.layers.xavier_initializer()),
        'b3': tf.get_variable('b3', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),
    }
    train_data = tf.cast(x, 'float32')
    conv = tf.nn.conv2d(train_data, weights['w1'], strides=[1, 1, 1, 1], use_cudnn_on_gpu=True, padding='SAME')
    conv = tf.nn.bias_add(conv, biases['b1'])
    activation1 = tf.nn.relu(conv)

    mean, var = tf.nn.moments(activation1, [0, 1, 2])
    offset = tf.Variable(tf.zeros([32]))
    scale = tf.Variable(tf.ones([32]))
    batch_norm = tf.nn.batch_normalization(activation1, mean, var, offset, scale, epsilon)

    max_pool = tf.nn.max_pool2d(batch_norm, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    flatten = tf.reshape(max_pool, [-1, weights['w2'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(flatten, weights['w2']), biases['b2'])
    activation2 = tf.nn.relu(fc1)
    fc2 = tf.add(tf.matmul(activation2, weights['w3']), biases['b3'])

    #prediction = tf.nn.softmax(fc2)
    CEloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(y, fc2))

    num_correct = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(num_correct, tf.float32))


    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(CEloss)

    return fc2, CEloss, accuracy, optimizer, weights, biases, x, y

"""## 2.2    Model Training"""

def SGD(trainData, validData, testData, batch_size, learning_rate, epoch, epsilon, beta1, beta2, train_target, valid_target, test_target):
  trainData = trainData.reshape(-1, 28, 28, 1)
  validData = validData.reshape(-1, 28, 28, 1)
  testData = testData.reshape(-1, 28, 28, 1)
  tf.reset_default_graph() 
  [prediction, CEloss, accuracy, optimizer, weights, biases, x, y] = CNN(trainData, train_target, learning_rate, beta1, beta2, epsilon)
  init_op = tf.global_variables_initializer()

  num_train_batch = trainData.shape[0] // batch_size

  train_loss = np.zeros(epoch)
  train_accuracy = np.zeros(epoch)
  valid_loss = np.zeros(epoch)
  valid_accuracy = np.zeros(epoch)
  test_loss = np.zeros(epoch)
  test_accuracy = np.zeros(epoch)
  
  sess = tf.InteractiveSession()
  sess.run(init_op)
  
  for i in range(epoch):
    train_idx = np.arange(trainData.shape[0])
    np.random.shuffle(train_idx)
    trainData = trainData[train_idx]
    train_target = train_target[train_idx]

    for j in range(num_train_batch):
      train_x_batch = trainData[j*batch_size:(j+1)*batch_size, :]
      train_y_batch = train_target[j*batch_size:(j+1)*batch_size, :]
      opt = sess.run(optimizer, feed_dict={x: train_x_batch, y: train_y_batch}) 
    train_loss_current, train_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x: trainData, y: train_target})
    train_loss[i] = train_loss_current
    train_accuracy[i] = train_accuracy_current
    
    valid_loss_current, valid_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x:validData, y:valid_target})
    valid_loss[i] = valid_loss_current
    valid_accuracy[i] = valid_accuracy_current

    test_loss_current, test_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x:testData, y:test_target})
    test_loss[i] = test_loss_current
    test_accuracy[i] = test_accuracy_current

    print("epoch =", i+1, "train loss: ", train_loss[i], "valid loss: ", valid_loss[i])  
    print("epoch =", i+1, "train accuracy: ", train_accuracy[i], "valid accuracy: ", valid_accuracy[i])  
    print("epoch =", i+1, "test accuracy: ", test_accuracy[i])
  sess.close()
  

  plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')
  plt.plot(range(len(train_loss)), valid_loss, 'g', label='Valid loss')
  plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')
  plt.title('Train, Valid and Test Loss with Adam optimizer')
  plt.xlabel('Epochs ',fontsize=16)
  plt.ylabel('Loss',fontsize=16)
  plt.legend()
  plt.figure()
  plt.show()

  plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')
  plt.plot(range(len(train_loss)), valid_accuracy, 'g', label='Valid Accuracy')
  plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')
  plt.title('Train, valid and Test Accuracy with Adam optimizer')
  plt.xlabel('Epochs ',fontsize=16)
  plt.ylabel('Accuracy',fontsize=16)
  plt.legend()
  plt.figure()
  plt.show()

SGD(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target)

"""## 2.3    Hyperparameter Investigation

### 2.3.1    L2 Regularization
"""

def CNN_with_reg(x, y, learning_rate, beta1, beta2, epsilon, lam):
    tf.set_random_seed(421)
    x = tf.placeholder("float", [None, 28,28,1])
    y = tf.placeholder("float", [None, 10])
    weights = {
        'w1': tf.get_variable('w1', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), 
        'w2': tf.get_variable('w2', shape=(14*14*32,784), initializer=tf.contrib.layers.xavier_initializer()),  
        'w3': tf.get_variable('w3', shape=(784,10), initializer=tf.contrib.layers.xavier_initializer()), 
    }
    biases = {
        'b1': tf.get_variable('b1', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),
        'b2': tf.get_variable('b2', shape=(784), initializer=tf.contrib.layers.xavier_initializer()),
        'b3': tf.get_variable('b3', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),
    }
    train_data = tf.cast(x, 'float32')
    conv = tf.nn.conv2d(train_data, weights['w1'], strides=[1, 1, 1, 1], use_cudnn_on_gpu=True, padding='SAME')
    conv = tf.nn.bias_add(conv, biases['b1'])
    activation1 = tf.nn.relu(conv)

    mean, var = tf.nn.moments(activation1, [0, 1, 2])
    offset = tf.Variable(tf.zeros([32]))
    scale = tf.Variable(tf.ones([32]))
    batch_norm = tf.nn.batch_normalization(activation1, mean, var, offset, scale, epsilon)

    max_pool = tf.nn.max_pool2d(batch_norm, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    flatten = tf.reshape(max_pool, [-1, weights['w2'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(flatten, weights['w2']), biases['b2'])
    activation2 = tf.nn.relu(fc1)
    fc2 = tf.add(tf.matmul(activation2, weights['w3']), biases['b3'])

    #prediction = tf.nn.softmax(fc2)
    reg = lam * (tf.nn.l2_loss(weights['w1']) + tf.nn.l2_loss(weights['w2']) + tf.nn.l2_loss(weights['w3']))
    CEloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(y, fc2)) + reg

    num_correct = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(num_correct, tf.float32))


    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(CEloss)

    return fc2, CEloss, accuracy, optimizer, weights, biases, x, y

def SGD_with_reg(trainData, validData, testData, batch_size, learning_rate, epoch, epsilon, beta1, beta2, train_target, valid_target, test_target, lam):
  trainData = trainData.reshape(-1, 28, 28, 1)
  validData = validData.reshape(-1, 28, 28, 1)
  testData = testData.reshape(-1, 28, 28, 1)
  tf.reset_default_graph() 
  [prediction, CEloss, accuracy, optimizer, weights, biases, x, y] = CNN_with_reg(trainData, train_target, learning_rate, beta1, beta2, epsilon, lam)
  init_op = tf.global_variables_initializer()

  num_train_batch = trainData.shape[0] // batch_size

  train_loss = np.zeros(epoch)
  train_accuracy = np.zeros(epoch)
  valid_loss = np.zeros(epoch)
  valid_accuracy = np.zeros(epoch)
  test_loss = np.zeros(epoch)
  test_accuracy = np.zeros(epoch)
  
  sess = tf.InteractiveSession()
  sess.run(init_op)
  
  for i in range(epoch):
    train_idx = np.arange(trainData.shape[0])
    np.random.shuffle(train_idx)
    trainData = trainData[train_idx]
    train_target = train_target[train_idx]

    for j in range(num_train_batch):
      train_x_batch = trainData[j*batch_size:(j+1)*batch_size, :]
      train_y_batch = train_target[j*batch_size:(j+1)*batch_size, :]
      opt = sess.run(optimizer, feed_dict={x: train_x_batch, y: train_y_batch}) 
    train_loss_current, train_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x: trainData, y: train_target})
    train_loss[i] = train_loss_current
    train_accuracy[i] = train_accuracy_current
    
    valid_loss_current, valid_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x:validData, y:valid_target})
    valid_loss[i] = valid_loss_current
    valid_accuracy[i] = valid_accuracy_current

    test_loss_current, test_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x:testData, y:test_target})
    test_loss[i] = test_loss_current
    test_accuracy[i] = test_accuracy_current

    print("epoch =", i+1, "train loss: ", train_loss[i], "valid loss: ", valid_loss[i])  
    print("epoch =", i+1, "train accuracy: ", train_accuracy[i], "valid accuracy: ", valid_accuracy[i]) 
    print("epoch =", i+1, "test accuracy: ", test_accuracy[i]) 
  sess.close()

  plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')
  plt.plot(range(len(train_loss)), valid_loss, 'g', label='Valid loss')
  plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')
  plt.title(f'Loss with reg = {lam}')
  plt.xlabel('Epochs ',fontsize=16)
  plt.ylabel('Loss',fontsize=16)
  plt.legend()
  plt.figure()
  plt.show()

  plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')
  plt.plot(range(len(train_loss)), valid_accuracy, 'g', label='Valid Accuracy')
  plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')
  plt.title(f'Accuracy with reg = {lam}')
  plt.xlabel('Epochs ',fontsize=16)
  plt.ylabel('Accuracy',fontsize=16)
  plt.legend()
  plt.figure()
  plt.show()

# lamda = 0.01
SGD_with_reg(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target, 0.01)

# lamda = 0.1
SGD_with_reg(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target, 0.1)

# lamda = 0.5
SGD_with_reg(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target, 0.5)

"""### 2.3.2    Dropout"""

def CNN_with_dropout(x, y, learning_rate, beta1, beta2, epsilon, keep_prob):
    tf.set_random_seed(421)
    x = tf.placeholder("float", [None, 28,28,1])
    y = tf.placeholder("float", [None, 10])
    weights = {
        'w1': tf.get_variable('w1', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), 
        'w2': tf.get_variable('w2', shape=(14*14*32,784), initializer=tf.contrib.layers.xavier_initializer()),  
        'w3': tf.get_variable('w3', shape=(784,10), initializer=tf.contrib.layers.xavier_initializer()), 
    }
    biases = {
        'b1': tf.get_variable('b1', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),
        'b2': tf.get_variable('b2', shape=(784), initializer=tf.contrib.layers.xavier_initializer()),
        'b3': tf.get_variable('b3', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),
    }
    train_data = tf.cast(x, 'float32')
    conv = tf.nn.conv2d(train_data, weights['w1'], strides=[1, 1, 1, 1], use_cudnn_on_gpu=True, padding='SAME')
    conv = tf.nn.bias_add(conv, biases['b1'])
    activation1 = tf.nn.relu(conv)

    mean, var = tf.nn.moments(activation1, [0, 1, 2])
    offset = tf.Variable(tf.zeros([32]))
    scale = tf.Variable(tf.ones([32]))
    batch_norm = tf.nn.batch_normalization(activation1, mean, var, offset, scale, epsilon)

    max_pool = tf.nn.max_pool2d(batch_norm, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    flatten = tf.reshape(max_pool, [-1, weights['w2'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(flatten, weights['w2']), biases['b2'])
    fc1 = tf.nn.dropout(fc1, keep_prob)
    activation2 = tf.nn.relu(fc1)
    fc2 = tf.add(tf.matmul(activation2, weights['w3']), biases['b3'])

    #prediction = tf.nn.softmax(fc2)
    CEloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(y, fc2))

    num_correct = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(num_correct, tf.float32))


    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(CEloss)

    return fc2, CEloss, accuracy, optimizer, weights, biases, x, y

def SGD_with_dropout(trainData, validData, testData, batch_size, learning_rate, epoch, epsilon, beta1, beta2, train_target, valid_target, test_target, keep_prob):
  trainData = trainData.reshape(-1, 28, 28, 1)
  validData = validData.reshape(-1, 28, 28, 1)
  testData = testData.reshape(-1, 28, 28, 1)
  tf.reset_default_graph() 
  [prediction, CEloss, accuracy, optimizer, weights, biases, x, y] = CNN_with_dropout(trainData, train_target, learning_rate, beta1, beta2, epsilon, keep_prob)
  init_op = tf.global_variables_initializer()

  num_train_batch = trainData.shape[0] // batch_size

  train_loss = np.zeros(epoch)
  train_accuracy = np.zeros(epoch)
  valid_loss = np.zeros(epoch)
  valid_accuracy = np.zeros(epoch)
  test_loss = np.zeros(epoch)
  test_accuracy = np.zeros(epoch)
  
  sess = tf.InteractiveSession()
  sess.run(init_op)
  
  for i in range(epoch):
    train_idx = np.arange(trainData.shape[0])
    np.random.shuffle(train_idx)
    trainData = trainData[train_idx]
    train_target = train_target[train_idx]

    for j in range(num_train_batch):
      train_x_batch = trainData[j*batch_size:(j+1)*batch_size, :]
      train_y_batch = train_target[j*batch_size:(j+1)*batch_size, :]
      opt = sess.run(optimizer, feed_dict={x: train_x_batch, y: train_y_batch}) 
    train_loss_current, train_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x: trainData, y: train_target})
    train_loss[i] = train_loss_current
    train_accuracy[i] = train_accuracy_current
    
    valid_loss_current, valid_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x:validData, y:valid_target})
    valid_loss[i] = valid_loss_current
    valid_accuracy[i] = valid_accuracy_current

    test_loss_current, test_accuracy_current = sess.run([CEloss, accuracy], feed_dict={x:testData, y:test_target})
    test_loss[i] = test_loss_current
    test_accuracy[i] = test_accuracy_current

    print("epoch =", i+1, "train loss: ", train_loss[i], "valid loss: ", valid_loss[i])  
    print("epoch =", i+1, "train accuracy: ", train_accuracy[i], "valid accuracy: ", valid_accuracy[i])  
    print("epoch =", i+1, "test accuracy: ", test_accuracy[i])
  sess.close()

  plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')
  plt.plot(range(len(train_loss)), valid_loss, 'g', label='Valid loss')
  plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')
  plt.title(f'Loss with p = {keep_prob}')
  plt.xlabel('Epochs ',fontsize=16)
  plt.ylabel('Loss',fontsize=16)
  plt.legend()
  plt.figure()
  plt.show()

  plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')
  plt.plot(range(len(train_loss)), valid_accuracy, 'g', label='Valid Accuracy')
  plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')
  plt.title(f'Accuracy with p = {keep_prob}')
  plt.xlabel('Epochs ',fontsize=16)
  plt.ylabel('Accuracy',fontsize=16)
  plt.legend()
  plt.figure()
  plt.show()

# keeping probability = 0.9
SGD_with_dropout(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target, 0.9)

# keeping probability = 0.75
SGD_with_dropout(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target, 0.75)

# keeping probability = 0.5
SGD_with_dropout(trainData, validData, testData, 32, 1e-4, 50, 1e-8, 0.9, 0.999, train_target, valid_target, test_target, 0.5)