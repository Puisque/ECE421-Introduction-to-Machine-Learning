# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sXfSdv21-6qBqlXmRi3bvcHpaN-CO2oC
"""

import tensorflow as tf
import numpy as np
from numpy import linalg
import matplotlib.pyplot as plt
import time

"""# Load Data"""

def loadData():
    with np.load('notMNIST.npz') as data :
        Data, Target = data ['images'], data['labels']
        posClass = 2
        negClass = 9
        dataIndx = (Target==posClass) + (Target==negClass)
        Data = Data[dataIndx]/255.
        Target = Target[dataIndx].reshape(-1, 1)
        Target[Target==posClass] = 1
        Target[Target==negClass] = 0
        np.random.seed(421)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data, Target = Data[randIndx], Target[randIndx]
        trainData, trainTarget = Data[:3500], Target[:3500]
        validData, validTarget = Data[3500:3600], Target[3500:3600]
        testData, testTarget = Data[3600:], Target[3600:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

[trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()
print(trainData.shape)
print(trainTarget.shape)

"""# Plotting functions"""

def plot_loss(train_loss, valid_loss):
    train_label = "Train"
    valid_label = "Validation"
    plt.title("Train vs Validation Loss")
    n = len(train_loss) # number of epochs
    plt.plot(range(1,n+1), train_loss, label=train_label)
    plt.plot(range(1,n+1), valid_loss, label=valid_label)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.grid()

def plot_test_loss(test_loss):
    plt.title("Test Loss")
    n = len(test_loss)
    plt.plot(range(1,n+1), test_loss)
    plt.xlabel("Number of examples")
    plt.ylabel("Loss")
    plt.grid()

def plot_accuracy(train_accuracy, valid_accuracy):
    train_label = "Train"
    valid_label = "Validation"
    plt.title("Train vs Validation Accuracy")
    n = len(train_accuracy) # number of epochs
    plt.plot(range(1,n+1), train_accuracy, label=train_label)
    plt.plot(range(1,n+1), valid_accuracy, label=valid_label)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.grid()

def plot_test_accuracy(test_accuracy):
    plt.title("Test Accuracy")
    n = len(test_accuracy)
    plt.plot(range(1,n+1), test_accuracy)
    plt.xlabel("Number of examples")
    plt.ylabel("Accuracy")
    plt.grid()

def plot_loss_alpha(train_loss, valid_loss, alpha):
    train_label = "Train with alpha = " + str(alpha)
    valid_label = "Validation with alpha = " + str(alpha)
    plt.title("Train vs Validation Loss with different Learning Rate")
    n = len(train_loss) # number of epochs
    plt.plot(range(1,n+1), train_loss, label=train_label)
    plt.plot(range(1,n+1), valid_loss, label=valid_label)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.grid()

def plot_loss_reg(train_loss, valid_loss, reg):
    train_label = "Train with reg = " + str(reg)
    valid_label = "Validation with reg = " + str(reg)
    plt.title("Train vs Validation Loss with different Regularization Parameter")
    n = len(train_loss) # number of epochs
    plt.plot(range(1,n+1), train_loss, label=train_label)
    plt.plot(range(1,n+1), valid_loss, label=valid_label)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.grid()

def plot_accuracy_alpha(train_accuracy, valid_accuracy, alpha):
    train_label = "Train with alpha = " + str(alpha)
    valid_label = "Validation with alpha = " + str(alpha)
    plt.title("Train vs Validation Accuracy with different Learning Rate")
    n = len(train_accuracy) # number of epochs
    plt.plot(range(1,n+1), train_accuracy, label=train_label)
    plt.plot(range(1,n+1), valid_accuracy, label=valid_label)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.grid()

def plot_accuracy_reg(train_accuracy, valid_accuracy, reg):
    train_label = "Train with reg = " + str(reg)
    valid_label = "Validation with reg = " + str(reg)
    plt.title("Train vs Validation Accuracy with different Regularization Parameter")
    n = len(train_accuracy) # number of epochs
    plt.plot(range(1,n+1), train_accuracy, label=train_label)
    plt.plot(range(1,n+1), valid_accuracy, label=valid_label)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.grid()

"""# Batch Grad descent function"""

def grad_descent(W, b, train_data, train_target, valid_data, valid_target, alpha, epochs, reg, error_tol, lossType="MSE"):
	start_time = time.time()
	train_loss = np.zeros(epochs)
	train_accuracy = np.zeros(epochs)
	valid_loss = np.zeros(epochs)
	valid_accuracy = np.zeros(epochs)

	if(lossType == "MSE"):
		for epoch in range(epochs):
			train_mse = MSE(W, b, train_data, train_target, reg)
			train_loss[epoch] = train_mse
			train_accuracy[epoch] = accMSE(W, b, train_data, train_target)

			valid_mse = MSE(W, b, valid_data, valid_target, reg)
			valid_loss[epoch] = valid_mse
			valid_accuracy[epoch] = accMSE(W, b, valid_data, valid_target)

			gradMSE_W, gradMSE_b = gradMSE(W, b, train_data, train_target, reg)
	 
			new_W = W - alpha * gradMSE_W
			new_b = b - alpha * gradMSE_b
			diff = np.linalg.norm(new_W - W)
			if diff < error_tol:
				return new_W,new_b
			else:
				W = new_W
				b = new_b

	else:
		for epoch in range(epochs):
			train_ce = crossEntropyLoss(W, b, train_data, train_target, reg)
			train_loss[epoch] = train_ce
			train_accuracy[epoch] = accCE(W, b, train_data, train_target)
	 
			valid_ce = crossEntropyLoss(W, b, valid_data, valid_target, reg)
			valid_loss[epoch] = valid_ce
			valid_accuracy[epoch] = accCE(W, b, valid_data, valid_target)
	 
			gradCE_W, gradCE_b = gradCE(W, b, train_data, train_target, reg)
			
			new_W = W - alpha * gradCE_W
			new_b = b - alpha * gradCE_b
			diff = np.linalg.norm(new_W - W)
			if diff < error_tol:
				return new_W,new_b
			else:
				W = new_W
				b = new_b

	end_time = time.time()
	diff_time = end_time - start_time
	print("GD elapse time: ", diff_time)
	return W, b, train_loss, valid_loss, train_accuracy, valid_accuracy

"""# Linear regression MSE

Calculation of loss, gradient and accuracy
"""

def MSE(W, b, x, y, reg):
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  err = np.dot(x,W) + b - y
  mse = np.sum(np.dot(err.T,err)) / x.shape[0] + (reg / 2) * np.dot(W.T,W)
  return mse

def gradMSE(W, b, x, y, reg):
  num_example = x.shape[0]
  num_dimension = x.shape[1] * x.shape[2]
  W = W.reshape((num_dimension, 1))
  x = x.reshape((num_example, num_dimension))
  err = np.dot(x,W) + b - y
  gradMSE_W = 2 * np.dot(x.T,err) / num_example + reg * W
  gradMSE_b = 2 * (np.sum(err)) / num_example
  return gradMSE_W, gradMSE_b

def accMSE(W, b, x, y):
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  mse_accuracy = np.sum((np.dot(x,W) + b > 0.5) == y) / x.shape[0]
  return mse_accuracy

def testMSE(W, b, x, y, reg):
  mse = 0
  test_loss = np.zeros(x.shape[0])
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  for idx in range(x.shape[0]):
    mse += (np.dot(x[idx][:],W) + b - y[idx]) ** 2
    test_loss[idx] = mse / (idx + 1) + (reg / 2) * np.dot(W.T,W)
  return test_loss

def testAccMSE(W, b, x, y):
  corr = 0
  test_accuracy = np.zeros(x.shape[0])
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  for idx in range(x.shape[0]):
    corr += (np.dot(x[idx][:],W) + b > 0.5) == y[idx]
    test_accuracy[idx] = corr / (idx + 1)
  return test_accuracy

"""Training for different learning rate and regularization"""

def train_linear_regression(epochs, alpha, reg):
  [trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()

  W = np.zeros([trainData.shape[1]*trainData.shape[2],1])
  b = 0
  [W, b, train_loss, valid_loss, train_accuracy, valid_accuracy] = grad_descent(W, b, trainData, trainTarget, validData, validTarget, alpha, epochs, reg, 10 **(-7))

  # Compute test loss and test accuracy
  test_loss = testMSE(W, b, testData, testTarget, reg)
  test_accuracy = testAccMSE(W, b, testData, testTarget)

  plot_loss(train_loss, valid_loss)
  plt.show()
  plot_accuracy(train_accuracy, valid_accuracy)
  plt.show()
  plot_test_loss(test_loss)
  plt.show()
  plot_test_accuracy(test_accuracy)
  plt.show()
  print("Final train loss: ", train_loss[epochs-1])
  print("Final train accuracy: ", train_accuracy[epochs-1])
  print("Final validation loss: ", valid_loss[epochs-1])
  print("Final validation accuracy: ", valid_accuracy[epochs-1])
  print("Final test loss: ", test_loss[testData.shape[0]-1])
  print("Final test accuracy: ", test_accuracy[testData.shape[0]-1])

train_linear_regression(5000, 0.005, 0)

train_linear_regression(5000, 0.001, 0)

train_linear_regression(5000, 0.0001, 0)

train_linear_regression(5000, 0.005, 0.001)

train_linear_regression(5000, 0.005, 0.1)

train_linear_regression(5000, 0.005, 0.5)

def train_alpha(epochs, reg):
  [trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()

  W1 = np.zeros([trainData.shape[1]*trainData.shape[2],1])
  W2 = np.zeros([trainData.shape[1]*trainData.shape[2],1])
  W3 = np.zeros([trainData.shape[1]*trainData.shape[2],1])
  
  b1 = 0
  b2 = 0
  b3 = 0

  [W1, b1,train_loss1, valid_loss1, train_accuracy1, valid_accuracy1] = grad_descent(W1, b1, trainData, trainTarget, validData, validTarget, 0.005, epochs, reg, 10 **(-7))
  [W2, b2,train_loss2, valid_loss2, train_accuracy2, valid_accuracy2] = grad_descent(W2, b2, trainData, trainTarget, validData, validTarget, 0.001, epochs, reg, 10 **(-7))
  [W3, b3,train_loss3, valid_loss3, train_accuracy3, valid_accuracy3] = grad_descent(W3, b3, trainData, trainTarget, validData, validTarget, 0.0001, epochs, reg, 10 **(-7))
  plot_loss_alpha(train_loss1, valid_loss1, 0.005)
  plot_loss_alpha(train_loss2, valid_loss2, 0.001)
  plot_loss_alpha(train_loss3, valid_loss3, 0.0001)
  plt.show()
  plot_accuracy_alpha(train_accuracy1, valid_accuracy1, 0.005)
  plot_accuracy_alpha(train_accuracy2, valid_accuracy2, 0.001)
  plot_accuracy_alpha(train_accuracy3, valid_accuracy3, 0.0001)
  plt.show()

  # test
  test_mse1 = MSE(W1,b1,testData,testTarget,reg)
  test_accuracy1 = accMSE(W1, b1, testData, testTarget)

  test_mse2 = MSE(W2,b2,testData,testTarget,reg)
  test_accuracy2 = accMSE(W2, b2, testData, testTarget)

  test_mse3 = MSE(W3,b3,testData,testTarget,reg)
  test_accuracy3 = accMSE(W3, b3, testData, testTarget)

  print(test_mse1[0][0])
  print(test_mse2[0][0])
  print(test_mse3[0][0])

  print(test_accuracy1)
  print(test_accuracy2)
  print(test_accuracy3)

train_alpha(5000, 0)

def train_reg(alpha, epochs):
  [trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()

  W1 = np.zeros([testData.shape[1]*testData.shape[2],1])
  W2 = np.zeros([testData.shape[1]*testData.shape[2],1])
  W3 = np.zeros([testData.shape[1]*testData.shape[2],1])
  
  b1 = 0
  b2 = 0
  b3 = 0

  [W1, b1,train_loss1, valid_loss1, train_accuracy1, valid_accuracy1] = grad_descent(W1, b1, trainData, trainTarget, validData, validTarget, alpha, epochs, 0.001, 10 **(-7))
  [W2, b2,train_loss2, valid_loss2, train_accuracy2, valid_accuracy2] = grad_descent(W2, b2, trainData, trainTarget, validData, validTarget, alpha, epochs, 0.1, 10 **(-7))
  [W3, b3,train_loss3, valid_loss3, train_accuracy3, valid_accuracy3] = grad_descent(W3, b3, trainData, trainTarget, validData, validTarget, alpha, epochs, 0.5, 10 **(-7))
  plot_loss_reg(train_loss1, valid_loss1, 0.001)
  plot_loss_reg(train_loss2, valid_loss2, 0.1)
  plot_loss_reg(train_loss3, valid_loss3, 0.5)
  plt.show()
  plot_accuracy_reg(train_accuracy1, valid_accuracy1, 0.001)
  plot_accuracy_reg(train_accuracy2, valid_accuracy2, 0.1)
  plot_accuracy_reg(train_accuracy3, valid_accuracy3, 0.5)
  plt.show()
  print("Final train accuracy for reg = 0.001: ", train_accuracy1[epochs-1])
  print("Final validation accuracy for reg = 0.001: ", valid_accuracy1[epochs-1])
  print("Final train accuracy for reg = 0.1: ", train_accuracy2[epochs-1])
  print("Final validation accuracy for reg = 0.1: ", valid_accuracy2[epochs-1])
  print("Final train accuracy for reg = 0.5: ", train_accuracy3[epochs-1])
  print("Final validation accuracy for reg = 0.5: ", valid_accuracy3[epochs-1])

  # test
  test_mse1 = MSE(W1,b1,testData,testTarget,0.001)
  test_accuracy1 = accMSE(W1, b1, testData, testTarget)

  test_mse2 = MSE(W2,b2,testData,testTarget,0.1)
  test_accuracy2 = accMSE(W2, b2, testData, testTarget)

  test_mse3 = MSE(W3,b3,testData,testTarget,0.5)
  test_accuracy3 = accMSE(W3, b3, testData, testTarget)

  print(test_mse1[0][0])
  print(test_mse2[0][0])
  print(test_mse3[0][0])

  print(test_accuracy1)
  print(test_accuracy2)
  print(test_accuracy3)

train_reg(0.005, 5000)

"""Normal equation calculation and comparison"""

def LeastSquaresSolution(x, y):
  num_example = x.shape[0]
  num_dimension = x.shape[1] * x.shape[2]
  x = x.reshape((num_example, num_dimension))
  print(x.shape)
  inverse = np.linalg.inv(np.dot(x.T, x))
  W = np.dot(np.dot(inverse, x.T), y)
  print(W.shape)
  return W

[trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()
start_time = time.time()
W = LeastSquaresSolution(trainData, trainTarget)
end_time = time.time()
diff_time = end_time - start_time
accuracy = accMSE(W, 0, trainData, trainTarget)
loss = MSE(W, 0, trainData, trainTarget, 0)
print("Normal equation loss: ", loss[0][0])
print("Normal equation ccuracy: ", accuracy)
print("Normal equation elapse time: ", diff_time)

train_linear_regression(5000, 0.005, 0)

"""# Logistic regression CE

Calculation of loss, gradient and accuracy
"""

def crossEntropyLoss(W, b, x, y, reg):
 if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
 y_hat = 1 / (1 + np.exp(-(np.dot(x,W) + b)))
 cross_entropy_loss = np.sum((-1)*y*np.log(y_hat) - (1-y)*np.log(1-y_hat)) / x.shape[0] + (reg / 2) * np.dot(W.T,W)
 return cross_entropy_loss

def gradCE(W, b, x, y, reg):
  num_example = x.shape[0]
  num_dimension = x.shape[1] * x.shape[2]
  W = W.reshape((num_dimension, 1))
  x = x.reshape((num_example, num_dimension))
  y_hat = 1 / (1 + np.exp(-(np.dot(x,W) + b)))
  gradCE_W = 2 * np.dot(x.T, (y_hat - y)) / num_example  + reg * W  # 784x1
  gradCE_b = 2 * np.sum(y_hat - y) / num_example
  return gradCE_W, gradCE_b

def accCE(W, b, x, y):
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  ce_accuracy = np.sum((1 / (1 + np.exp(-(np.dot(x,W) + b))) > 0.5) == y) / x.shape[0]
  return ce_accuracy

def testCE(W, b, x, y, reg):
  ce = 0
  test_loss = np.zeros(x.shape[0])
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  for idx in range(x.shape[0]):
    y_hat = 1 / (1 + np.exp(-(np.dot(x[idx][:],W) + b)))
    ce += (-1) * y[idx] * np.log(y_hat) - (1 - y[idx]) * np.log(1 - y_hat)
    test_loss[idx] = ce / (idx + 1) + (reg / 2) * np.dot(W.T,W)
  return test_loss

def testAccCE(W, b, x, y):
  corr = 0
  test_accuracy = np.zeros(x.shape[0])
  if len(x.shape)>2:
   num_example = x.shape[0]
   num_dimension = x.shape[1] * x.shape[2]
   W = W.reshape((num_dimension, 1))
   x = x.reshape((num_example, num_dimension))
  for idx in range(x.shape[0]):
    corr += (1 / (1 + np.exp(-(np.dot(x[idx][:],W) + b))) > 0.5) == y[idx]
    test_accuracy[idx] = corr / (idx + 1)
  return test_accuracy

"""Training logistic regression model"""

def train_ce(alpha, epochs, reg):
  [trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()

  W = np.zeros([trainData.shape[1]*trainData.shape[2],1])
  
  b = 0

  [W, b, train_loss, valid_loss, train_accuracy, valid_accuracy] = grad_descent(W, b, trainData, trainTarget, validData, validTarget, alpha, epochs, reg, 10 **(-7), "CE")
  

  # Compute test loss and test accuracy
  test_loss = testCE(W, b, testData, testTarget, reg)
  test_accuracy = testAccCE(W, b, testData, testTarget)

  # Compute test loss and test accuracy
  test_loss = testCE(W, b, testData, testTarget, reg)
  test_accuracy = testAccCE(W, b, testData, testTarget)

  plot_loss(train_loss, valid_loss)
  plt.show()
  plot_accuracy(train_accuracy, valid_accuracy)
  plt.show()
  plot_test_loss(test_loss)
  plt.show()
  plot_test_accuracy(test_accuracy)
  plt.show()
  print("Final train loss: ", train_loss[epochs-1])
  print("Final train accuracy: ", train_accuracy[epochs-1])
  print("Final validation loss: ", valid_loss[epochs-1])
  print("Final validation accuracy: ", valid_accuracy[epochs-1])
  print("Final test loss: ", test_loss[testData.shape[0]-1])
  print("Final test accuracy: ", test_accuracy[testData.shape[0]-1])

train_ce(0.005, 5000, 0.1)

train_ce(0.005, 5000, 0)

"""Comparison with linear regression model"""

train_linear_regression(5000, 0.005, 0)

"""# Stochastic Gradient Descent

Initialization function
"""

def buildGraph(batch_size, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):
  #Initialize weight and bias tensors
  tf.set_random_seed(421)

  W = tf.Variable(tf.truncated_normal(shape=(28*28, 1), stddev=0.5, dtype=tf.float32))
  b = tf.Variable(tf.truncated_normal(shape=(), stddev=0.5, dtype=tf.float32))
  
  reg = tf.placeholder(dtype=tf.float32, shape=(), name='reg')
  x = tf.placeholder(dtype=tf.float32, shape=(batch_size, 28*28), name='x')
  y = tf.placeholder(dtype=tf.float32, shape=(batch_size, 1), name='y')

  if loss_type == "MSE":
  # Your implementation
    predicted_labels = tf.add(tf.matmul(x, W), b)
    loss = tf.losses.mean_squared_error(y, predicted_labels) + (reg / 2) * tf.matmul(tf.transpose(W),W)

  elif loss_type == "CE":
  #Your implementation here
    #predicted_labels = tf.sigmoid(tf.add(tf.matmul(x, W), b))
    predicted_labels = tf.add(tf.matmul(x, W), b)
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = predicted_labels) \
                          + (reg / 2) * tf.matmul(tf.transpose(W),W))

  optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)

  return W, b, predicted_labels, y, x, loss, optimizer, reg

"""SGD implementation"""

def SGD(batch_size, epoch, lam, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):

  [W, b, predicted_labels, y, x, loss, optimizer, reg] = buildGraph(batch_size, loss_type, learning_rate, beta1, beta2, epsilon)
  
  init_op = tf.global_variables_initializer()

  [trainData, validData, testData, trainTarget, validTarget, testTarget] = loadData()
  trainData = trainData.reshape(np.shape(trainData)[0],np.shape(trainData)[1]*np.shape(trainData)[2])
  validData = validData.reshape(np.shape(validData)[0],np.shape(validData)[1]*np.shape(validData)[2])
  testData = testData.reshape(np.shape(testData)[0],np.shape(testData)[1]*np.shape(testData)[2])

  num_train_batch = trainData.shape[0] // batch_size

  train_loss = np.zeros(epoch)
  train_accuracy = np.zeros(epoch)
  valid_loss = np.zeros(epoch)
  valid_accuracy = np.zeros(epoch)
  
  
  sess = tf.InteractiveSession()
  sess.run(init_op)
  
  for i in range(epoch):
    train_idx = np.arange(trainData.shape[0])
    np.random.shuffle(train_idx)
    trainData = trainData[train_idx, :]
    trainTarget = trainTarget[train_idx]
    for j in range(num_train_batch):
      train_x_batch = trainData[j*batch_size:(j+1)*batch_size, :]
      train_y_batch = trainTarget[j*batch_size:(j+1)*batch_size, :]
      _, current_loss, current_W, current_b, y_pred = sess.run([optimizer, loss, W, b, predicted_labels],\
                                                               feed_dict={x:train_x_batch, y:train_y_batch, reg:lam})
    if loss_type == "MSE":
      train_mse = MSE(current_W, current_b, train_x_batch, train_y_batch, 0)
      train_loss[i] = train_mse
      train_accuracy[i] = accMSE(current_W, current_b, train_x_batch, train_y_batch)
      valid_mse = MSE(current_W, current_b, validData, validTarget, 0)
      valid_loss[i] = valid_mse
      valid_accuracy[i] = accMSE(current_W, current_b, validData, validTarget)
    elif loss_type == "CE":
      train_ce = crossEntropyLoss(current_W, current_b, train_x_batch, train_y_batch, 0)
      train_loss[i] = train_ce
      train_accuracy[i] = accCE(current_W, current_b, train_x_batch, train_y_batch)
      valid_ce = crossEntropyLoss(current_W, current_b, validData, validTarget, 0)
      valid_loss[i] = valid_ce
      valid_accuracy[i] = accCE(current_W, current_b, validData, validTarget)
  
  sess.close()
  plot_loss(train_loss, valid_loss)
  plt.show()
  plot_accuracy(train_accuracy, valid_accuracy)
  plt.show()
  
  if loss_type == "MSE":
    test_loss = testMSE(current_W, current_b, testData, testTarget, 0)
    test_accuracy = testAccMSE(current_W, current_b, testData, testTarget)
    
  elif loss_type == "CE":
    test_loss = testCE(current_W, current_b, testData, testTarget, 0)
    test_accuracy = testAccCE(current_W, current_b, testData, testTarget)

  plot_test_loss(test_loss)
  plt.show()
  plot_test_accuracy(test_accuracy)
  plt.show()
  print("Test loss is: ", test_loss[144])
  print("Test accuracy is: ", test_accuracy[144])

"""# SGD with MSE"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""SGD Batch size investigation

Batch size = 100
"""

SGD(100, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Batch size = 700"""

SGD(700, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Batch size = 1750"""

SGD(1750, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Hyperparameter investigation

beta1  = 0.95
"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.95, beta2=0.999, epsilon=1e-08)

"""beta1  = 0.99"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.99, beta2=0.999, epsilon=1e-08)

"""beta2  = 0.99"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.99, epsilon=1e-08)

"""beta2 = 0.9999"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.9999, epsilon=1e-08)

"""epsilon = 1e-9"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-09)

"""epsilon = 1e-4"""

SGD(500, 700, 0, loss_type="MSE", learning_rate = 0.001, beta1=0.9, beta2=0.9999, epsilon=1e-04)

"""# SGD with CE"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Batch size investigations

Batch size = 100
"""

SGD(100, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Batch size = 700"""

SGD(700, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Batch size = 1750"""

SGD(1750, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)

"""Hyperparameter investigation

beta1 = 0.95
"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.95, beta2=0.999, epsilon=1e-08)

"""beta1 = 0.99"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.99, beta2=0.999, epsilon=1e-08)

"""beta2 = 0.99"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.99, epsilon=1e-08)

"""beta2 = 0.9999"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.9999, epsilon=1e-08)

"""epsilon = 1e-9"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-09)

"""epsilon = 1e-4"""

SGD(500, 700, 0, loss_type="CE", learning_rate = 0.001, beta1=0.9, beta2=0.999, epsilon=1e-04)