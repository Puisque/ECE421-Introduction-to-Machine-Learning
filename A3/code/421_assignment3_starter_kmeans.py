# -*- coding: utf-8 -*-
"""starter_kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wU4bk1KBuMfUAC3SLZ6y5yhnk4LRzAyo
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import helper as hlp

# Loading data
#data = np.load('data2D.npy')
data = np.load('data100D.npy')
[num_pts, dim] = np.shape(data)

# For Validation set
if is_valid:
  valid_batch = int(num_pts / 3.0)
  np.random.seed(45689)
  rnd_idx = np.arange(num_pts)
  np.random.shuffle(rnd_idx)
  val_data = data[rnd_idx[:valid_batch]]
  data = data[rnd_idx[valid_batch:]]

# Distance function for K-means
def distanceFunc(X, MU):
  # Inputs
  # X: is an NxD matrix (N observations and D dimensions)
  # MU: is an KxD matrix (K means and D dimensions)
  # Outputs
  # pair_dist: is the squared pairwise distance matrix (NxK)
  # TODO
  X_X = tf.reshape(tf.reduce_sum(tf.square(X), axis=1), [-1, 1])
  MU_MU = tf.reshape(tf.reduce_sum(tf.square(MU), axis=1), [1, -1])
  X_MU = (-2) * tf.matmul(X, tf.transpose(MU))
  return X_X + MU_MU + X_MU

# K = k, N = num_pts, D = dim
def kmeans(k, learning_rate, beta1, beta2, epsilon):
  tf.set_random_seed(421)
  K = k
  N = num_pts
  D = dim

  X = tf.placeholder(tf.float32, shape=(None,D), name="X")
  MU = tf.get_variable("MU", initializer=tf.random.normal(shape=[K,D]))

  dist = distanceFunc(X,MU)
  loss = tf.reduce_sum(tf.reduce_min(dist, axis=1))
  optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)

  return X, MU, dist, loss, optimizer

def train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=3, is_valid=False):
  if is_valid:
    valid_batch = int(num_pts / 3.0)
    np.random.seed(45689)
    rnd_idx = np.arange(num_pts)
    np.random.shuffle(rnd_idx)
    val_data = data[rnd_idx[:valid_batch]]
    train_data = data[rnd_idx[valid_batch:]]
  else:
    train_data = data

  train_loss = np.zeros(epoch)
  valid_loss = np.zeros(epoch)

  tf.reset_default_graph() 
  [X, MU, dist, loss, optimizer] = kmeans(k, learning_rate, beta1, beta2, epsilon)

  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    for i in range(epoch):
      train_current_MU,train_current_dist,train_current_loss,_ = sess.run([MU,dist,loss,optimizer], feed_dict={X:train_data})
      train_loss[i] = train_current_loss

      if is_valid:
          valid_current_MU,valid_current_dist,valid_current_loss,_ = sess.run([MU,dist,loss,optimizer], feed_dict={X:val_data})
          valid_loss[i] = valid_current_loss

    final_MU = train_current_MU
    final_dist = train_current_dist
  
  clusters = np.argmin(train_current_dist, axis=1)
  for i in range(k):
    print("For class: ", i, "percentage is: ", np.sum(i == clusters)*100.0/len(clusters))
  
  print("Final Mu is: ", final_MU)
  print("Final Training Loss is: ", train_loss[epoch-1])
  print("Final Validation Loss is: ", valid_loss[epoch-1])
  plt.figure(1)
  plt.plot(range(len(train_loss)),train_loss,label="Train Loss")
  #plt.plot(range(len(train_loss)),valid_loss,label="Validation Loss")
  plt.legend(loc = "best")
  plt.title('K-Means Train Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.show()

  k = len(final_MU)
  plt.scatter(train_data[:, 0], train_data[:, 1], c=clusters, 
              cmap=plt.get_cmap('Set3'), s=25, alpha=0.6)
  plt.scatter(final_MU[:, 0], final_MU[:, 1], marker='*', c="black", 
              cmap=plt.get_cmap('Set1'), s=50, linewidths=1)
  plt.title('K-Means Clusters')
  plt.xlabel('X')
  plt.ylabel('Y')
  plt.grid()
  plt.show()
  
  return valid_loss

k3 = train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=3, is_valid=False)

train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=1, is_valid=False)

train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=2, is_valid=False)

train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=4, is_valid=False)

train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=5, is_valid=False)

k1_valid_loss = train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=1, is_valid=True)

k2_valid_loss = train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=2, is_valid=True)

k3_valid_loss = train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=3, is_valid=True)

k4_valid_loss = train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=4, is_valid=True)

k5_valid_loss = train(learning_rate=0.1, epoch=500, epsilon=1e-5, beta1=0.9, beta2=0.99, k=5, is_valid=True)

plt.plot(range(len(k1_valid_loss)),k1_valid_loss,label="k=1")
plt.plot(range(len(k2_valid_loss)),k2_valid_loss,label="k=2")
plt.plot(range(len(k3_valid_loss)),k3_valid_loss,label="k=3")
plt.plot(range(len(k4_valid_loss)),k4_valid_loss,label="k=4")
plt.plot(range(len(k5_valid_loss)),k5_valid_loss,label="k=5")
plt.legend(loc = "best")
plt.title('K-Means Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

#part 2
k5_valid_loss = train(learning_rate=0.1, epoch=1000, k=5, is_valid=True)
k10_valid_loss = train(learning_rate=0.1, epoch=1000, k=10, is_valid=True)
k15_valid_loss = train(learning_rate=0.1, epoch=1000, k=15, is_valid=True)
k20_valid_loss = train(learning_rate=0.1, epoch=1000, k=20, is_valid=True)
k30_valid_loss = train(learning_rate=0.1, epoch=1000, k=30, is_valid=True)

plt.plot(range(len(k5_valid_loss)),k5_valid_loss,label="k=5")
plt.plot(range(len(k10_valid_loss)),k10_valid_loss,label="k=10")
plt.plot(range(len(k15_valid_loss)),k15_valid_loss,label="k=15")
plt.plot(range(len(k20_valid_loss)),k20_valid_loss,label="k=20")
plt.plot(range(len(k30_valid_loss)),k20_valid_loss,label="k=30")
plt.legend(loc = "best")
plt.title('K-Means Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()